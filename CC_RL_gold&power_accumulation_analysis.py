# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ekipFN6JDGr9A3KDToFEwAilT1G-Z_-M
"""

import random
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque

# --- HYPERPARAMETERS ---
GAMMA = 0.99
LEARNING_RATE = 0.001
EPSILON = 1.0
EPSILON_DECAY = 0.995
EPSILON_MIN = 0.1
BATCH_SIZE = 32
MEMORY_SIZE = 5000

# --- DEEP Q-NETWORK ---
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# --- REPLAY MEMORY ---
class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

# --- CHARACTER CLASSES ---
class Knight:
    def __init__(self, name, duke):
        self.name = name
        self.gold = 1
        self.power = 2
        self.training_boost = False
        self.equipment_purchased = False
        self.duke = duke

    def take_action(self):
        action = random.choice(["Duel", "Train", "Mission", "Steal Gold", "Buy Equipment"])
        if action == "Duel":
            self.power += 3 if random.random() < 0.55 else 0
        elif action == "Train":
            self.training_boost = True
        elif action == "Mission":
            if random.random() < 0.75:
                self.gold += 14
                self.power += 2
        elif action == "Steal Gold":
            self.gold += 5
        elif action == "Buy Equipment" and self.gold >= 8:
            self.gold -= 8
            self.equipment_purchased = True

class RL_Knight:
    def __init__(self, name, duke, model, target_model, optimizer, memory):
        self.name = name
        self.gold = 1
        self.power = 2
        self.training_boost = False
        self.equipment_purchased = False
        self.duke = duke
        self.model = model
        self.target_model = target_model
        self.optimizer = optimizer
        self.memory = memory
        self.state_size = 4  # gold, power, training, equipment
        self.action_size = 5  # Duel, Train, Mission, Steal Gold, Buy Equipment

    def get_state(self):
        return np.array([self.gold, self.power, int(self.training_boost), int(self.equipment_purchased)], dtype=np.float32)

    def take_action(self):
        state = torch.tensor(self.get_state(), dtype=torch.float32).unsqueeze(0)
        if random.random() < EPSILON:
            action = random.randint(0, self.action_size - 1)
        else:
            with torch.no_grad():
                action = self.model(state).argmax().item()

        reward = 0
        if action == 0:  # Duel
            self.power += 3 if random.random() < 0.55 else 0
            reward = 5
        elif action == 1:  # Train
            self.training_boost = True
            reward = 2
        elif action == 2:  # Mission
            if random.random() < 0.75:
                self.gold += 14
                self.power += 2
                reward = 6
        elif action == 3:  # Steal Gold
            self.gold += 5
            reward = 4
        elif action == 4:  # Buy Equipment
            if self.gold >= 8 and not self.equipment_purchased:
                self.gold -= 8
                self.equipment_purchased = True
                reward = 3

        next_state = torch.tensor(self.get_state(), dtype=torch.float32).unsqueeze(0)
        self.memory.push(state, action, reward, next_state, False)
        return reward

# --- TRAINING SETUP ---
rl_model = DQN(4, 5)
target_model = DQN(4, 5)
target_model.load_state_dict(rl_model.state_dict())
target_model.eval()
optimizer = optim.Adam(rl_model.parameters(), lr=LEARNING_RATE)
memory = ReplayMemory(MEMORY_SIZE)

# --- SIMULATION ---
coup_success_count_rl = 0
assassination_success_count_rl = 0
gold_accumulation = {f"Knight_{i+1}": [] for i in range(12)}
power_accumulation = {f"Knight_{i+1}": [] for i in range(12)}
gold_accumulation["RL_Knight"] = []
power_accumulation["RL_Knight"] = []

for simulation_run in range(50):
    dukes = [f"Duke_{i+1}" for i in range(6)]
    knights = [Knight(f"Knight_{i+1}", random.choice(dukes)) for i in range(11)]
    rl_knight = RL_Knight("RL_Knight", random.choice(dukes), rl_model, target_model, optimizer, memory)
    knights.append(rl_knight)

    for turn in range(50):
        for knight in knights:
            knight.take_action()
            gold_accumulation[knight.name].append(knight.gold)
            power_accumulation[knight.name].append(knight.power)

# --- GRAPHING RESULTS ---
plt.figure(figsize=(12, 6))
for name, gold_values in gold_accumulation.items():
    plt.plot(gold_values, label=name, linestyle='-' if name == "RL_Knight" else '--', alpha=0.6)
plt.xlabel('Turns')
plt.ylabel('Gold Accumulation')
plt.title('Gold Accumulation Over Time (RL vs. Normal Knights)')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
for name, power_values in power_accumulation.items():
    plt.plot(power_values, label=name, linestyle='-' if name == "RL_Knight" else '--', alpha=0.6)
plt.xlabel('Turns')
plt.ylabel('Power Accumulation')
plt.title('Power Accumulation Over Time (RL vs. Normal Knights)')
plt.legend()
plt.show()